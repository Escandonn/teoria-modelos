
## ¬øQu√© es DiffusionPipeline?
Es una herramienta que combina varios modelos de inteligencia artificial (como UNET, codificadores de texto, etc.) en una sola interfaz f√°cil de usar para generar im√°genes.

## C√≥mo cargar un pipeline

### M√©todo 1: Usar la clase general
```python
import torch
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "Qwen/Qwen-Image", 
    torch_dtype=torch.bfloat16, 
    device_map="cuda"
)
```

### M√©todo 2: Usar la clase espec√≠fica
```python
import torch
from diffusers import QwenImagePipeline

pipeline = QwenImagePipeline.from_pretrained(
    "Qwen/Qwen-Image", 
    torch_dtype=torch.bfloat16, 
    device_map="cuda"
)
```

## Tipos de pipelines disponibles

| Pipeline | Funci√≥n |
|----------|---------|
| QwenImagePipeline | Texto a imagen |
| QwenImageImg2ImgPipeline | Imagen a imagen |
| QwenImageInpaintPipeline | Relleno de im√°genes |

## C√≥mo usar modelos locales

### Paso 1: Descargar el modelo
```python
from huggingface_hub import snapshot_download

snapshot_download(repo_id="Qwen/Qwen-Image")
```

### Paso 2: Cargar desde tu computadora
```python
pipeline = QwenImagePipeline.from_pretrained(
    "ruta/a/tu/cache", 
    torch_dtype=torch.bfloat16, 
    device_map="cuda"
)
```

## Control de precisi√≥n
Puedes cargar modelos con diferentes niveles de precisi√≥n para ahorrar memoria:

```python
# Todos los modelos en bfloat16
pipeline = QwenImagePipeline.from_pretrained(
    "Qwen/Qwen-Image", 
    torch_dtype=torch.bfloat16
)

# Modelos espec√≠ficos con diferente precisi√≥n
pipeline = QwenImagePipeline.from_pretrained(
    "Qwen/Qwen-Image",
    torch_dtype={"transformer": torch.bfloat16, "default": torch.float16}
)
```

## Configuraci√≥n de dispositivos

### Opciones para device_map:
- **"cuda"**: Usa la GPU
- **"balanced"**: Distribuye entre varias GPUs

```python
pipeline = DiffusionPipeline.from_pretrained(
    "Qwen/Qwen-Image", 
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)
```

## Carga r√°pida para modelos grandes
```python
import os
os.environ["HF_ENABLE_PARALLEL_LOADING"] = "YES"

pipeline = DiffusionPipeline.from_pretrained(
    "modelo-grande", 
    torch_dtype=torch.bfloat16, 
    device_map="cuda"
)
```

## Personalizaci√≥n avanzada

### Cambiar componentes
```python
from diffusers import AutoModel

# Usar un VAE mejorado
vae = AutoModel.from_pretrained(
    "madebyollin/sdxl-vae-fp16-fix", 
    torch_dtype=torch.float16
)

pipeline = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    vae=vae,
    torch_dtype=torch.float16,
    device_map="cuda"
)
```

### Reutilizar modelos entre pipelines
```python
from diffusers import AutoPipelineForText2Image

# Primer pipeline
pipeline_sdxl = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", 
    torch_dtype=torch.float16, 
    device_map="cuda"
)

# Segundo pipeline que reusa modelos
pipeline = AutoPipelineForText2Image.from_pipe(
    pipeline_sdxl, 
    enable_pag=True
)
```

## Seguridad
Para desactivar el filtro de contenido (no recomendado para aplicaciones p√∫blicas):
```python
pipeline = DiffusionPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5", 
    safety_checker=None
)
```

-------------------------------------------
-------------------------------------------
-------------------------------------------
-------------------------------------------


Aqu√≠ tienes el **tutorial en espa√±ol** sobre **DiffusionPipeline en Diffusers** explicado de forma clara y pr√°ctica.

---

# üß† ¬øQu√© es `DiffusionPipeline`?

Los modelos de difusi√≥n est√°n formados por varios componentes:

* UNet o DiT (modelo de difusi√≥n)
* Text Encoder
* VAE (autoencoder)
* Scheduler

`DiffusionPipeline` **envuelve todos esos componentes en una sola API f√°cil de usar**, pero sin quitarte la posibilidad de modificarlos individualmente.

Es la forma **oficial y moderna** de cargar modelos en Diffusers.

---

# üöÄ Cargar un pipeline

`DiffusionPipeline` detecta autom√°ticamente qu√© clase de pipeline usar leyendo el archivo `model_index.json` del modelo.

```python
import torch
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "Qwen/Qwen-Image",
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)
```

---

## üß© Usar la subclase espec√≠fica del pipeline

Cada modelo tiene subclases especializadas:

| Subclase                   | Tarea           |
| -------------------------- | --------------- |
| `QwenImagePipeline`        | texto ‚Üí imagen  |
| `QwenImageImg2ImgPipeline` | imagen ‚Üí imagen |
| `QwenImageInpaintPipeline` | inpainting      |

Puedes cargarlas directamente:

```python
from diffusers import QwenImagePipeline

pipeline = QwenImagePipeline.from_pretrained(
    "Qwen/Qwen-Image",
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)
```

---

# üíæ Ejecutar el modelo localmente (sin volver a descargar)

Descarga el modelo al cach√©:

```python
from huggingface_hub import snapshot_download

snapshot_download(repo_id="Qwen/Qwen-Image")
```

Luego c√°rgalo desde la ruta local:

```python
pipeline = QwenImagePipeline.from_pretrained(
    "ruta/a/tu/cache",
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)
```

---

# üéöÔ∏è Tipos de datos (`torch_dtype`)

Reducir la precisi√≥n baja el consumo de VRAM.

### Un solo tipo para todo:

```python
pipeline = QwenImagePipeline.from_pretrained(
    "Qwen/Qwen-Image",
    torch_dtype=torch.bfloat16
)
```

### Diferente precisi√≥n por componente:

```python
pipeline = QwenImagePipeline.from_pretrained(
    "Qwen/Qwen-Image",
    torch_dtype={
        "transformer": torch.bfloat16,
        "default": torch.float16
    }
)
```

---

# üñ•Ô∏è Ubicaci√≥n en dispositivos (`device_map`)

| Opci√≥n       | Descripci√≥n               |
| ------------ | ------------------------- |
| `"cuda"`     | Coloca todo en GPU        |
| `"balanced"` | Distribuye en varias GPUs |

```python
pipeline = DiffusionPipeline.from_pretrained(
    "Qwen/Qwen-Image",
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)
```

Ver d√≥nde qued√≥ cada parte:

```python
print(pipeline.hf_device_map)
```

---

## üîÑ Resetear `device_map`

Necesario si usar√°s:

* `.to()`
* `enable_model_cpu_offload()`
* `enable_sequential_cpu_offload()`

```python
pipeline.reset_device_map()
```

---

# ‚ö° Carga paralela (MUY IMPORTANTE para modelos grandes)

Acelera much√≠simo la carga.

```python
import os
os.environ["HF_ENABLE_PARALLEL_LOADING"] = "YES"

pipeline = DiffusionPipeline.from_pretrained(
    "Wan-AI/Wan2.2-I2V-A14B-Diffusers",
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)
```

---

# üîÅ Reemplazar partes del pipeline (ej: VAE mejorado)

```python
from diffusers import DiffusionPipeline, AutoModel

vae = AutoModel.from_pretrained(
    "madebyollin/sdxl-vae-fp16-fix",
    torch_dtype=torch.float16
)

pipeline = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    vae=vae,
    torch_dtype=torch.float16,
    device_map="cuda"
)
```

---

# ‚ôªÔ∏è Reutilizar modelos en varios pipelines (ahorra VRAM)

```python
from diffusers import AutoPipelineForText2Image

pipeline_sdxl = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    device_map="cuda"
)

# Crear otro pipeline usando los mismos pesos
pipeline = AutoPipelineForText2Image.from_pipe(
    pipeline_sdxl,
    enable_pag=True
)
```

üëâ Ambos usan **la misma memoria**.

‚ö†Ô∏è Comparten estado: si modificas uno, afecta al otro.

---

# üõ°Ô∏è Safety Checker (filtro de contenido)

Puedes desactivarlo (no recomendado en apps p√∫blicas):

```python
pipeline = DiffusionPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5",
    safety_checker=None
)
```

---

# üß† Lo m√°s importante que debes recordar

1. **Siempre usa `DiffusionPipeline` o `AutoPipeline`**
2. Usa `torch_dtype` para ahorrar VRAM
3. Usa `device_map="cuda"`
4. Activa `HF_ENABLE_PARALLEL_LOADING`
5. Puedes cambiar VAE, schedulers, etc.
6. Puedes reutilizar modelos sin gastar m√°s memoria

---

Si quieres, en el siguiente mensaje puedo darte el **template profesional base para Colab** que sirve para **cualquier modelo Diffusers (imagen, video, I2V, T2V, inpaint, etc.)** sin que vuelvas a tener errores de versiones.
